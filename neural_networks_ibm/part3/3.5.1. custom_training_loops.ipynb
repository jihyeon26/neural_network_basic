{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8077c844",
   "metadata": {},
   "source": [
    "## Custom Training Loops in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204fbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a8b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5d3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5b3af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3743958473205566\n",
      "Epoch 1 Step 200: Loss = 0.39980998635292053\n",
      "Epoch 1 Step 400: Loss = 0.1824227124452591\n",
      "Epoch 1 Step 600: Loss = 0.190804585814476\n",
      "Epoch 1 Step 800: Loss = 0.21089746057987213\n",
      "Epoch 1 Step 1000: Loss = 0.48431193828582764\n",
      "Epoch 1 Step 1200: Loss = 0.1345197856426239\n",
      "Epoch 1 Step 1400: Loss = 0.26571205258369446\n",
      "Epoch 1 Step 1600: Loss = 0.23788246512413025\n",
      "Epoch 1 Step 1800: Loss = 0.15216906368732452\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0945412814617157\n",
      "Epoch 2 Step 200: Loss = 0.1728932112455368\n",
      "Epoch 2 Step 400: Loss = 0.11890389025211334\n",
      "Epoch 2 Step 600: Loss = 0.08480465412139893\n",
      "Epoch 2 Step 800: Loss = 0.10181252658367157\n",
      "Epoch 2 Step 1000: Loss = 0.28182512521743774\n",
      "Epoch 2 Step 1200: Loss = 0.05708758533000946\n",
      "Epoch 2 Step 1400: Loss = 0.16776081919670105\n",
      "Epoch 2 Step 1600: Loss = 0.1877271980047226\n",
      "Epoch 2 Step 1800: Loss = 0.08635355532169342\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a66e7",
   "metadata": {},
   "source": [
    "### Adding Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ee75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c5c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b6a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd99587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3632686138153076 Accuracy = 0.1875\n",
      "Epoch 1 Step 200: Loss = 0.4184502363204956 Accuracy = 0.8347325921058655\n",
      "Epoch 1 Step 400: Loss = 0.1777428388595581 Accuracy = 0.8687655925750732\n",
      "Epoch 1 Step 600: Loss = 0.1555330902338028 Accuracy = 0.8844113945960999\n",
      "Epoch 1 Step 800: Loss = 0.20300383865833282 Accuracy = 0.8966526389122009\n",
      "Epoch 1 Step 1000: Loss = 0.3931822180747986 Accuracy = 0.9040022492408752\n",
      "Epoch 1 Step 1200: Loss = 0.15781570971012115 Accuracy = 0.9107774496078491\n",
      "Epoch 1 Step 1400: Loss = 0.1900905966758728 Accuracy = 0.915662944316864\n",
      "Epoch 1 Step 1600: Loss = 0.21526694297790527 Accuracy = 0.9188007712364197\n",
      "Epoch 1 Step 1800: Loss = 0.14350934326648712 Accuracy = 0.922751247882843\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08563232421875 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.17792275547981262 Accuracy = 0.961442768573761\n",
      "Epoch 2 Step 400: Loss = 0.10861668735742569 Accuracy = 0.9585411548614502\n",
      "Epoch 2 Step 600: Loss = 0.04933779686689377 Accuracy = 0.9606904983520508\n",
      "Epoch 2 Step 800: Loss = 0.10719242691993713 Accuracy = 0.961532473564148\n",
      "Epoch 2 Step 1000: Loss = 0.25937527418136597 Accuracy = 0.9621316194534302\n",
      "Epoch 2 Step 1200: Loss = 0.09621845185756683 Accuracy = 0.9629475474357605\n",
      "Epoch 2 Step 1400: Loss = 0.09366306662559509 Accuracy = 0.9639766216278076\n",
      "Epoch 2 Step 1600: Loss = 0.19460386037826538 Accuracy = 0.9635774493217468\n",
      "Epoch 2 Step 1800: Loss = 0.0637468621134758 Accuracy = 0.9643427133560181\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.04016488790512085 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.12546822428703308 Accuracy = 0.9752798676490784\n",
      "Epoch 3 Step 400: Loss = 0.08923524618148804 Accuracy = 0.9737375378608704\n",
      "Epoch 3 Step 600: Loss = 0.04412694275379181 Accuracy = 0.9747816324234009\n",
      "Epoch 3 Step 800: Loss = 0.061800483614206314 Accuracy = 0.9749922156333923\n",
      "Epoch 3 Step 1000: Loss = 0.14530712366104126 Accuracy = 0.9752435088157654\n",
      "Epoch 3 Step 1200: Loss = 0.07222045958042145 Accuracy = 0.975255012512207\n",
      "Epoch 3 Step 1400: Loss = 0.057137809693813324 Accuracy = 0.9755531549453735\n",
      "Epoch 3 Step 1600: Loss = 0.1421133428812027 Accuracy = 0.9752693772315979\n",
      "Epoch 3 Step 1800: Loss = 0.028675859794020653 Accuracy = 0.9756906032562256\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.024741558358073235 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.07127364724874496 Accuracy = 0.981965184211731\n",
      "Epoch 4 Step 400: Loss = 0.07152751088142395 Accuracy = 0.9805174469947815\n",
      "Epoch 4 Step 600: Loss = 0.04749159887433052 Accuracy = 0.9811252355575562\n",
      "Epoch 4 Step 800: Loss = 0.0463484451174736 Accuracy = 0.9811953902244568\n",
      "Epoch 4 Step 1000: Loss = 0.09297940880060196 Accuracy = 0.9814248085021973\n",
      "Epoch 4 Step 1200: Loss = 0.055117908865213394 Accuracy = 0.9814477562904358\n",
      "Epoch 4 Step 1400: Loss = 0.03513317182660103 Accuracy = 0.9817317724227905\n",
      "Epoch 4 Step 1600: Loss = 0.08929521590471268 Accuracy = 0.9816520810127258\n",
      "Epoch 4 Step 1800: Loss = 0.019274599850177765 Accuracy = 0.9820065498352051\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.016051216050982475 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.03791474550962448 Accuracy = 0.9866293668746948\n",
      "Epoch 5 Step 400: Loss = 0.06903189420700073 Accuracy = 0.9857387542724609\n",
      "Epoch 5 Step 600: Loss = 0.029663048684597015 Accuracy = 0.9863768815994263\n",
      "Epoch 5 Step 800: Loss = 0.022826289758086205 Accuracy = 0.9862671494483948\n",
      "Epoch 5 Step 1000: Loss = 0.05679934471845627 Accuracy = 0.9865759015083313\n",
      "Epoch 5 Step 1200: Loss = 0.041228484362363815 Accuracy = 0.9867298007011414\n",
      "Epoch 5 Step 1400: Loss = 0.031433720141649246 Accuracy = 0.986884355545044\n",
      "Epoch 5 Step 1600: Loss = 0.06980261206626892 Accuracy = 0.9867660999298096\n",
      "Epoch 5 Step 1800: Loss = 0.016450773924589157 Accuracy = 0.987003743648529\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cbc181",
   "metadata": {},
   "source": [
    "### Custom Callback for Advanced Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa7199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cecd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3a5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca1719ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f85b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3990375995635986 Accuracy = 0.125\n",
      "Epoch 1 Step 200: Loss = 0.38822653889656067 Accuracy = 0.8392412662506104\n",
      "Epoch 1 Step 400: Loss = 0.20411020517349243 Accuracy = 0.8675966262817383\n",
      "Epoch 1 Step 600: Loss = 0.18734461069107056 Accuracy = 0.8819155693054199\n",
      "Epoch 1 Step 800: Loss = 0.15861785411834717 Accuracy = 0.8939996957778931\n",
      "Epoch 1 Step 1000: Loss = 0.4058365821838379 Accuracy = 0.9011925458908081\n",
      "Epoch 1 Step 1200: Loss = 0.18550646305084229 Accuracy = 0.9075250029563904\n",
      "Epoch 1 Step 1400: Loss = 0.2811054587364197 Accuracy = 0.9122948050498962\n",
      "Epoch 1 Step 1600: Loss = 0.27342554926872253 Accuracy = 0.9158728718757629\n",
      "Epoch 1 Step 1800: Loss = 0.1698465347290039 Accuracy = 0.9196279644966125\n",
      "End of epoch 1, loss: 0.03275144845247269, accuracy: 0.9216333627700806\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.0906425416469574 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.18414810299873352 Accuracy = 0.9609763622283936\n",
      "Epoch 2 Step 400: Loss = 0.10539618134498596 Accuracy = 0.9583073854446411\n",
      "Epoch 2 Step 600: Loss = 0.08850221335887909 Accuracy = 0.9593386054039001\n",
      "Epoch 2 Step 800: Loss = 0.08237367868423462 Accuracy = 0.9604790806770325\n",
      "Epoch 2 Step 1000: Loss = 0.26030558347702026 Accuracy = 0.9611950516700745\n",
      "Epoch 2 Step 1200: Loss = 0.12242458760738373 Accuracy = 0.9615424871444702\n",
      "Epoch 2 Step 1400: Loss = 0.1265367716550827 Accuracy = 0.9624821543693542\n",
      "Epoch 2 Step 1600: Loss = 0.23121041059494019 Accuracy = 0.9627381563186646\n",
      "Epoch 2 Step 1800: Loss = 0.0948580875992775 Accuracy = 0.9634057283401489\n",
      "End of epoch 2, loss: 0.016941336914896965, accuracy: 0.9640666842460632\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8986883",
   "metadata": {},
   "source": [
    "### Add Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "641e7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e169aa",
   "metadata": {},
   "source": [
    "### Define the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1005d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98154738",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3aa137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a705874",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c405a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26c665",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caf8801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5074 - loss: 0.6950 \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5248 - loss: 0.6921\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5101 - loss: 0.6903\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5703 - loss: 0.6856\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5300 - loss: 0.6883 \n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6060 - loss: 0.6817\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5323 - loss: 0.6812 \n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6261 - loss: 0.6738\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5627 - loss: 0.6812\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2042878e450>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05449725",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faec95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5162 - loss: 0.7041  \n",
      "Test loss: 0.7079513072967529\n",
      "Test accuracy: 0.48500001430511475\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c063d3",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic Custom Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88f81e",
   "metadata": {},
   "source": [
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "714f2475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss= 0.034112349152565\n",
      "Epoch 2: Loss= 0.03041655756533146\n",
      "Epoch 3: Loss= 0.03094700165092945\n",
      "Epoch 4: Loss= 0.0417451485991478\n",
      "Epoch 5: Loss= 0.023672671988606453\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape = (28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training =True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    print(f'Epoch {epoch+1}: Loss= {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aa6c3",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61bf5778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss= 0.0343487411737442 Accuracy = 0.9238499999046326\n",
      "Epoch 2: Loss= 0.055059000849723816 Accuracy = 0.9661166667938232\n",
      "Epoch 3: Loss= 0.06592924147844315 Accuracy = 0.9775166511535645\n",
      "Epoch 4: Loss= 0.05747043713927269 Accuracy = 0.9836000204086304\n",
      "Epoch 5: Loss= 0.021500930190086365 Accuracy = 0.9884499907493591\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape = (28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training =True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        accuracy_metric.update_state(y_batch, logits)\n",
    "    print(f'Epoch {epoch+1}: Loss= {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e2cda",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72434ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.03475452959537506, accuracy: 0.9239333271980286\n",
      "End of epoch 2, loss: 0.0480518564581871, accuracy: 0.9646999835968018\n",
      "End of epoch 3, loss: 0.08060084283351898, accuracy: 0.9763166904449463\n",
      "End of epoch 4, loss: 0.038986414670944214, accuracy: 0.9831833243370056\n",
      "End of epoch 5, loss: 0.027860203757882118, accuracy: 0.9876999855041504\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'End of epoch {epoch+1}, loss: {logs.get('loss')}, accuracy: {logs.get('accuracy')}')\n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c889870",
   "metadata": {},
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ff9ff",
   "metadata": {},
   "source": [
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'End of epoch {epoch+1}, loss: {logs.get('loss')}, accuracy: {logs.get('accuracy')}')\n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
