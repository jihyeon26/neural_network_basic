{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\Desktop\n",
            "c:\\Users\\USER\n",
            "train loss:2.2918682351586783\n",
            "=== epoch:1, train acc:0.242, test acc:0.229 ===\n",
            "train loss:2.278420890045613\n",
            "train loss:2.268272499957663\n",
            "train loss:2.2424896615051413\n",
            "train loss:2.187616359148015\n",
            "train loss:2.1737100877099698\n",
            "train loss:2.0891502002253945\n",
            "train loss:2.02662615984145\n",
            "train loss:1.964604872925731\n",
            "train loss:1.767824190914378\n",
            "train loss:1.7657002272614515\n",
            "train loss:1.6611172847016429\n",
            "train loss:1.4621143752749697\n",
            "train loss:1.3707881579607366\n",
            "train loss:1.2698498780152792\n",
            "train loss:1.0889443187543357\n",
            "train loss:0.9906241201396385\n",
            "train loss:0.8329340122111357\n",
            "train loss:0.8219530421857891\n",
            "train loss:0.9005672545038762\n",
            "train loss:0.5979890789947817\n",
            "train loss:0.7342898073831974\n",
            "train loss:0.6920672075259106\n",
            "train loss:0.7242576313847654\n",
            "train loss:0.559665725995329\n",
            "train loss:0.5189404084833698\n",
            "train loss:0.6088797446912336\n",
            "train loss:0.5496358575108371\n",
            "train loss:0.4071146405897471\n",
            "train loss:0.41613145185302314\n",
            "train loss:0.41147351288483264\n",
            "train loss:0.3280370364079108\n",
            "train loss:0.3888115075686535\n",
            "train loss:0.639359373910527\n",
            "train loss:0.5821231973001059\n",
            "train loss:0.4297740527588411\n",
            "train loss:0.3122880265492592\n",
            "train loss:0.5350670128935515\n",
            "train loss:0.37635739395133605\n",
            "train loss:0.5476883780762438\n",
            "train loss:0.3134284983012672\n",
            "train loss:0.3214177425974966\n",
            "train loss:0.3566669725347764\n",
            "train loss:0.40690050774859976\n",
            "train loss:0.25970249790983346\n",
            "train loss:0.5776843037246808\n",
            "train loss:0.44209064842585344\n",
            "train loss:0.35976286783623534\n",
            "train loss:0.38772581759379293\n",
            "train loss:0.31608275228425003\n",
            "train loss:0.4652909371623729\n",
            "=== epoch:2, train acc:0.866, test acc:0.84 ===\n",
            "train loss:0.3718703004325912\n",
            "train loss:0.40687673087338455\n",
            "train loss:0.3635864789796164\n",
            "train loss:0.5463020072743955\n",
            "train loss:0.5057571664257311\n",
            "train loss:0.2662169176816368\n",
            "train loss:0.30003352382603543\n",
            "train loss:0.5010410408824142\n",
            "train loss:0.3945612024950669\n",
            "train loss:0.21483603529528644\n",
            "train loss:0.4190462573056214\n",
            "train loss:0.3396525135631612\n",
            "train loss:0.38658522268583956\n",
            "train loss:0.27558008697996184\n",
            "train loss:0.25900372213068573\n",
            "train loss:0.32739755578722723\n",
            "train loss:0.5253445378025631\n",
            "train loss:0.3420398368688568\n",
            "train loss:0.3458357910632822\n",
            "train loss:0.3543141111037983\n",
            "train loss:0.31553088899720466\n",
            "train loss:0.4223121793289893\n",
            "train loss:0.3269487318070172\n",
            "train loss:0.24288351749990222\n",
            "train loss:0.2569637381128258\n",
            "train loss:0.19258997805054118\n",
            "train loss:0.36194594186718326\n",
            "train loss:0.29628835675468707\n",
            "train loss:0.30077510949249686\n",
            "train loss:0.3378994468402135\n",
            "train loss:0.18730205102706268\n",
            "train loss:0.3149475314483192\n",
            "train loss:0.3531594761196844\n",
            "train loss:0.19669731905788723\n",
            "train loss:0.32408275711921514\n",
            "train loss:0.2268400416521506\n",
            "train loss:0.24417929610815828\n",
            "train loss:0.2649003094289933\n",
            "train loss:0.23545449415017095\n",
            "train loss:0.19648949735699117\n",
            "train loss:0.15063647339990355\n",
            "train loss:0.32009562551579657\n",
            "train loss:0.34929620350029855\n",
            "train loss:0.3558765098646706\n",
            "train loss:0.36743216080754443\n",
            "train loss:0.36187497322972395\n",
            "train loss:0.28670735650888285\n",
            "train loss:0.19168575590208747\n",
            "train loss:0.26527173319937936\n",
            "train loss:0.24750504968588596\n",
            "=== epoch:3, train acc:0.917, test acc:0.9 ===\n",
            "train loss:0.17673796374082226\n",
            "train loss:0.21121807931639272\n",
            "train loss:0.23682829628871768\n",
            "train loss:0.2521562859945593\n",
            "train loss:0.07835154849179066\n",
            "train loss:0.4269410765194857\n",
            "train loss:0.21763801979527886\n",
            "train loss:0.2055119312130733\n",
            "train loss:0.1101029632163095\n",
            "train loss:0.14479079057378125\n",
            "train loss:0.31715739022802203\n",
            "train loss:0.21995716237870389\n",
            "train loss:0.16933808763718644\n",
            "train loss:0.2575190975780999\n",
            "train loss:0.3049130675926566\n",
            "train loss:0.19237664359535564\n",
            "train loss:0.19832798480918448\n",
            "train loss:0.3221933019938391\n",
            "train loss:0.19338196252785816\n",
            "train loss:0.16550379636633888\n",
            "train loss:0.2236180550781631\n",
            "train loss:0.26326548549051654\n",
            "train loss:0.2040281025458036\n",
            "train loss:0.18677870411241393\n",
            "train loss:0.20099343208439435\n",
            "train loss:0.20104660813343347\n",
            "train loss:0.21401628977925327\n",
            "train loss:0.13125071628577617\n",
            "train loss:0.11999762746214286\n",
            "train loss:0.24845513432260236\n",
            "train loss:0.20923937808098075\n",
            "train loss:0.22238151044493212\n",
            "train loss:0.1842247869328086\n",
            "train loss:0.09372792003562448\n",
            "train loss:0.16961103633412886\n",
            "train loss:0.278736428311673\n",
            "train loss:0.18488447217250045\n",
            "train loss:0.14532698623227763\n",
            "train loss:0.13500103623286136\n",
            "train loss:0.1342780687203578\n",
            "train loss:0.13310413023515014\n",
            "train loss:0.2302452319069021\n",
            "train loss:0.14044946614646472\n",
            "train loss:0.20953036243961423\n",
            "train loss:0.31298583102992994\n",
            "train loss:0.12530948378559292\n",
            "train loss:0.1877861351684282\n",
            "train loss:0.10814254396224418\n",
            "train loss:0.1395666756445779\n",
            "train loss:0.18535043985241206\n",
            "=== epoch:4, train acc:0.945, test acc:0.93 ===\n",
            "train loss:0.12746394883219964\n",
            "train loss:0.09283265489049866\n",
            "train loss:0.06613863704318955\n",
            "train loss:0.06691549076550087\n",
            "train loss:0.21000737357500185\n",
            "train loss:0.236772489023436\n",
            "train loss:0.14376863036725707\n",
            "train loss:0.0706351032576495\n",
            "train loss:0.11482852361685197\n",
            "train loss:0.15652476711028085\n",
            "train loss:0.21393561387299706\n",
            "train loss:0.2822874179371962\n",
            "train loss:0.20885888208092893\n",
            "train loss:0.1692940294927362\n",
            "train loss:0.18449956953876803\n",
            "train loss:0.10175042711223468\n",
            "train loss:0.10478343922217417\n",
            "train loss:0.1888793219006654\n",
            "train loss:0.24219893400388018\n",
            "train loss:0.11300758270499708\n",
            "train loss:0.12814007620366627\n",
            "train loss:0.08554674043261898\n",
            "train loss:0.13256476887633098\n",
            "train loss:0.15336478963297004\n",
            "train loss:0.11601743688095886\n",
            "train loss:0.17486332585262157\n",
            "train loss:0.1324548217275799\n",
            "train loss:0.15675346559626527\n",
            "train loss:0.10690788832340534\n",
            "train loss:0.14154722389821608\n",
            "train loss:0.05222698295066293\n",
            "train loss:0.1812445316024342\n",
            "train loss:0.1408720618148336\n",
            "train loss:0.11471138786214748\n",
            "train loss:0.19541826861164735\n",
            "train loss:0.14929917221202213\n",
            "train loss:0.07646839231860253\n",
            "train loss:0.05720755242215328\n",
            "train loss:0.12624739845522026\n",
            "train loss:0.060504171843955304\n",
            "train loss:0.12801143155808692\n",
            "train loss:0.14710494701348023\n",
            "train loss:0.1243246782247701\n",
            "train loss:0.1703041963787471\n",
            "train loss:0.10123772065632189\n",
            "train loss:0.12354357379732306\n",
            "train loss:0.12455639377678139\n",
            "train loss:0.16612946220166971\n",
            "train loss:0.11493174205814213\n",
            "train loss:0.21894238200923685\n",
            "=== epoch:5, train acc:0.946, test acc:0.941 ===\n",
            "train loss:0.22947897164143127\n",
            "train loss:0.10684931710502905\n",
            "train loss:0.10840863217186264\n",
            "train loss:0.17854197159847207\n",
            "train loss:0.10280570867458864\n",
            "train loss:0.24997090573383599\n",
            "train loss:0.133669666845912\n",
            "train loss:0.04003629062395463\n",
            "train loss:0.1596822355408528\n",
            "train loss:0.09084002919417271\n",
            "train loss:0.12097681621247394\n",
            "train loss:0.15937272794351606\n",
            "train loss:0.19238973075704982\n",
            "train loss:0.12821184782640901\n",
            "train loss:0.07562522355128169\n",
            "train loss:0.08637041758115055\n",
            "train loss:0.09967522204262579\n",
            "train loss:0.09056751203494283\n",
            "train loss:0.14890248570679374\n",
            "train loss:0.04711805773317696\n",
            "train loss:0.20362195739861305\n",
            "train loss:0.1069059596163187\n",
            "train loss:0.1424894289850287\n",
            "train loss:0.15410054634849576\n",
            "train loss:0.05244346805279667\n",
            "train loss:0.21453380019003865\n",
            "train loss:0.10231016358086548\n",
            "train loss:0.1336534883603851\n",
            "train loss:0.07570542165056744\n",
            "train loss:0.20058876549423255\n",
            "train loss:0.17015623736382918\n",
            "train loss:0.19846178980666945\n",
            "train loss:0.10853772821320228\n",
            "train loss:0.10854943726089314\n",
            "train loss:0.07651098179190893\n",
            "train loss:0.08683323200152407\n",
            "train loss:0.08847246839275083\n",
            "train loss:0.09851670618297646\n",
            "train loss:0.17655205723711867\n",
            "train loss:0.10400580560636248\n",
            "train loss:0.13624700095362735\n",
            "train loss:0.07905160839640951\n",
            "train loss:0.04766958104572233\n",
            "train loss:0.07261162102947089\n",
            "train loss:0.060165024342509296\n",
            "train loss:0.10777982262927288\n",
            "train loss:0.12328624351586093\n",
            "train loss:0.13099192083903283\n",
            "train loss:0.10207625915180252\n",
            "train loss:0.1237140286042777\n",
            "=== epoch:6, train acc:0.965, test acc:0.952 ===\n",
            "train loss:0.09344455421459634\n",
            "train loss:0.06647240487157986\n",
            "train loss:0.08569872544950594\n",
            "train loss:0.08656645584263478\n",
            "train loss:0.038204269274199974\n",
            "train loss:0.06565733865707818\n",
            "train loss:0.06551917078669998\n",
            "train loss:0.07673858806789625\n",
            "train loss:0.07301815176288824\n",
            "train loss:0.08453536582125329\n",
            "train loss:0.056095383709285594\n",
            "train loss:0.055595438850114746\n",
            "train loss:0.07388828214677046\n",
            "train loss:0.04388848198016399\n",
            "train loss:0.056523482258044384\n",
            "train loss:0.08224841152483478\n",
            "train loss:0.03817502098276235\n",
            "train loss:0.0972701996073336\n",
            "train loss:0.11565690576080492\n",
            "train loss:0.07199289614074587\n",
            "train loss:0.16254093553046567\n",
            "train loss:0.1123479609819041\n",
            "train loss:0.057880422985908474\n",
            "train loss:0.0700577245832012\n",
            "train loss:0.04999380174147496\n",
            "train loss:0.038197262179896545\n",
            "train loss:0.08676907909763619\n",
            "train loss:0.04918252352623975\n",
            "train loss:0.08381012753214058\n",
            "train loss:0.07018561027437395\n",
            "train loss:0.030339779407500073\n",
            "train loss:0.05579396169883297\n",
            "train loss:0.041751204724390305\n",
            "train loss:0.048469429985722526\n",
            "train loss:0.11036060594696273\n",
            "train loss:0.07518400891559614\n",
            "train loss:0.08015737828901127\n",
            "train loss:0.07700497880302545\n",
            "train loss:0.06801401570666245\n",
            "train loss:0.04944282613376446\n",
            "train loss:0.08367665075270937\n",
            "train loss:0.10316551911305048\n",
            "train loss:0.07654487231140428\n",
            "train loss:0.05727596074841079\n",
            "train loss:0.06398123207330748\n",
            "train loss:0.11015982332423001\n",
            "train loss:0.05519016242052212\n",
            "train loss:0.027966511416408425\n",
            "train loss:0.14013729837517463\n",
            "train loss:0.05176746533171094\n",
            "=== epoch:7, train acc:0.968, test acc:0.955 ===\n",
            "train loss:0.1107526746034429\n",
            "train loss:0.06538645274730052\n",
            "train loss:0.08359518090830566\n",
            "train loss:0.06096542962196523\n",
            "train loss:0.04626879237365673\n",
            "train loss:0.1034919298098989\n",
            "train loss:0.07202592989524677\n",
            "train loss:0.02316303948769151\n",
            "train loss:0.10715111559746023\n",
            "train loss:0.05098405436445615\n",
            "train loss:0.055900126450412015\n",
            "train loss:0.04905781494626299\n",
            "train loss:0.041900154772569614\n",
            "train loss:0.04184275926694954\n",
            "train loss:0.06045706362157406\n",
            "train loss:0.04290355233627517\n",
            "train loss:0.0705854818795396\n",
            "train loss:0.1087276331597925\n",
            "train loss:0.053814521049237014\n",
            "train loss:0.034537346389059066\n",
            "train loss:0.035812005257414764\n",
            "train loss:0.03299698167579788\n",
            "train loss:0.04772681527154429\n",
            "train loss:0.025968466897966693\n",
            "train loss:0.037865099199621104\n",
            "train loss:0.02898252418413173\n",
            "train loss:0.06299611811148532\n",
            "train loss:0.1394271772760124\n",
            "train loss:0.02677504247048301\n",
            "train loss:0.046083865960298065\n",
            "train loss:0.029709105136446767\n",
            "train loss:0.03711591570615275\n",
            "train loss:0.21415343138111215\n",
            "train loss:0.031804029681260705\n",
            "train loss:0.04102788339413989\n",
            "train loss:0.055021754891753726\n",
            "train loss:0.05042174171128516\n",
            "train loss:0.023748974455789384\n",
            "train loss:0.05472681269070062\n",
            "train loss:0.033396802462172796\n",
            "train loss:0.04984964898747814\n",
            "train loss:0.027146678409111\n",
            "train loss:0.06473954284450403\n",
            "train loss:0.06025871264793388\n",
            "train loss:0.03379039607419451\n",
            "train loss:0.02401349278890073\n",
            "train loss:0.025160074322826756\n",
            "train loss:0.033674755480003246\n",
            "train loss:0.027363372932862947\n",
            "train loss:0.03337905643408074\n",
            "=== epoch:8, train acc:0.982, test acc:0.958 ===\n",
            "train loss:0.08063382729525284\n",
            "train loss:0.04659864406419925\n",
            "train loss:0.0672320583910342\n",
            "train loss:0.048876074088763814\n",
            "train loss:0.06768316814352314\n",
            "train loss:0.042922379521259595\n",
            "train loss:0.08207469220261389\n",
            "train loss:0.059157161491504705\n",
            "train loss:0.027458212120900886\n",
            "train loss:0.14973044284573983\n",
            "train loss:0.011016843769719927\n",
            "train loss:0.05130264205296764\n",
            "train loss:0.09358627761271811\n",
            "train loss:0.03970573482273461\n",
            "train loss:0.038067506849521926\n",
            "train loss:0.055782767676804186\n",
            "train loss:0.03470796583900452\n",
            "train loss:0.08592613767390402\n",
            "train loss:0.08887051961308194\n",
            "train loss:0.021366512358810437\n",
            "train loss:0.022177322880077677\n",
            "train loss:0.06454887821747055\n",
            "train loss:0.09757409419396643\n",
            "train loss:0.022121486878828554\n",
            "train loss:0.039038713312633526\n",
            "train loss:0.034686102413272774\n",
            "train loss:0.03029585511193529\n",
            "train loss:0.04295644185387628\n",
            "train loss:0.06853533869790697\n",
            "train loss:0.0734208783737575\n",
            "train loss:0.042449208941122585\n",
            "train loss:0.032635166935613545\n",
            "train loss:0.05166761740309298\n",
            "train loss:0.03628259364480117\n",
            "train loss:0.13474219965339915\n",
            "train loss:0.018266120855458645\n",
            "train loss:0.030129243325849157\n",
            "train loss:0.02999401806193398\n",
            "train loss:0.08618330107368917\n",
            "train loss:0.03858276496571639\n",
            "train loss:0.024453342333921473\n",
            "train loss:0.07016450678566326\n",
            "train loss:0.07144654780818992\n",
            "train loss:0.012971755106106105\n",
            "train loss:0.03581899181527466\n",
            "train loss:0.018360047264588793\n",
            "train loss:0.06377200795314955\n",
            "train loss:0.026189357484906684\n",
            "train loss:0.025338209360553907\n",
            "train loss:0.025784882171029658\n",
            "=== epoch:9, train acc:0.98, test acc:0.962 ===\n",
            "train loss:0.027962129204640182\n",
            "train loss:0.05445724247200332\n",
            "train loss:0.02677981099973422\n",
            "train loss:0.036926814741474794\n",
            "train loss:0.09571286161384666\n",
            "train loss:0.034732850736466536\n",
            "train loss:0.018525533771774336\n",
            "train loss:0.10995920365718524\n",
            "train loss:0.006801469670462341\n",
            "train loss:0.027971623169484698\n",
            "train loss:0.0795778370208739\n",
            "train loss:0.0468633233233467\n",
            "train loss:0.0649922048543165\n",
            "train loss:0.029972628121820735\n",
            "train loss:0.0420869909552529\n",
            "train loss:0.02057899278577286\n",
            "train loss:0.05629818421375852\n",
            "train loss:0.02468137649233508\n",
            "train loss:0.03231489778515348\n",
            "train loss:0.043587489838444876\n",
            "train loss:0.03402443849541279\n",
            "train loss:0.030139651580452873\n",
            "train loss:0.026203216671527953\n",
            "train loss:0.024959453713037337\n",
            "train loss:0.0338018496697695\n",
            "train loss:0.04751251521192499\n",
            "train loss:0.03263124884758086\n",
            "train loss:0.02796579481226339\n",
            "train loss:0.05305172921000732\n",
            "train loss:0.02382649074812915\n",
            "train loss:0.015287739275554513\n",
            "train loss:0.011203895408597522\n",
            "train loss:0.021740447527376854\n",
            "train loss:0.009872278845615068\n",
            "train loss:0.03239177597510043\n",
            "train loss:0.057623651865435\n",
            "train loss:0.044041363622161926\n",
            "train loss:0.015105315123871962\n",
            "train loss:0.0379671161376838\n",
            "train loss:0.034300102839454547\n",
            "train loss:0.04287114454438308\n",
            "train loss:0.032474980098911\n",
            "train loss:0.027787887298550227\n",
            "train loss:0.014248262793925784\n",
            "train loss:0.07709107923667041\n",
            "train loss:0.014208129191712564\n",
            "train loss:0.041370691376998836\n",
            "train loss:0.05317946703524172\n",
            "train loss:0.031175299916874778\n",
            "train loss:0.0075636107118801175\n",
            "=== epoch:10, train acc:0.985, test acc:0.965 ===\n",
            "train loss:0.05481843186854882\n",
            "train loss:0.016853527803648526\n",
            "train loss:0.03332031603342395\n",
            "train loss:0.029354516870853317\n",
            "train loss:0.02702195458269022\n",
            "train loss:0.019542541381305564\n",
            "train loss:0.032318117667300646\n",
            "train loss:0.012984287750061914\n",
            "train loss:0.027016646843400233\n",
            "train loss:0.018053086546930736\n",
            "train loss:0.014163517279493793\n",
            "train loss:0.06397167281051405\n",
            "train loss:0.02441478944315429\n",
            "train loss:0.029152179411414517\n",
            "train loss:0.03155954197156124\n",
            "train loss:0.01229518876445036\n",
            "train loss:0.01007317028771228\n",
            "train loss:0.03286760462801064\n",
            "train loss:0.01981052300855463\n",
            "train loss:0.020980598478589046\n",
            "train loss:0.07076593354923395\n",
            "train loss:0.014981481687585556\n",
            "train loss:0.031717562240688196\n",
            "train loss:0.017857023762207058\n",
            "train loss:0.06635552215634656\n",
            "train loss:0.036628453944528794\n",
            "train loss:0.029800498356970014\n",
            "train loss:0.017024587657277334\n",
            "train loss:0.01762180999672455\n",
            "train loss:0.010168127225429386\n",
            "train loss:0.021304135035809614\n",
            "train loss:0.01509080196011092\n",
            "train loss:0.013752572883128937\n",
            "train loss:0.0513951214823908\n",
            "train loss:0.0075720679803548555\n",
            "train loss:0.021772281496326738\n",
            "train loss:0.012649058374645498\n",
            "train loss:0.018359190023258046\n",
            "train loss:0.01347178246355953\n",
            "train loss:0.03491772615679081\n",
            "train loss:0.011189352846957268\n",
            "train loss:0.03406598267294146\n",
            "train loss:0.017895696661334545\n",
            "train loss:0.029241283802846737\n",
            "train loss:0.02267311692972665\n",
            "train loss:0.00882039156225668\n",
            "train loss:0.18439162373244763\n",
            "train loss:0.016466448530230684\n",
            "train loss:0.021607433235659987\n",
            "train loss:0.011613680037340453\n",
            "=== epoch:11, train acc:0.991, test acc:0.963 ===\n",
            "train loss:0.02432185181336426\n",
            "train loss:0.03242481671907845\n",
            "train loss:0.024387045541311454\n",
            "train loss:0.012594722793109107\n",
            "train loss:0.007832803766775878\n",
            "train loss:0.04041239637612203\n",
            "train loss:0.02119962960120192\n",
            "train loss:0.010413744209791555\n",
            "train loss:0.02141536333065003\n",
            "train loss:0.02090229059873067\n",
            "train loss:0.015526832677352389\n",
            "train loss:0.015432361418944078\n",
            "train loss:0.016541118013697287\n",
            "train loss:0.016908573035616214\n",
            "train loss:0.007859125036690477\n",
            "train loss:0.021549568513286505\n",
            "train loss:0.018639962970361496\n",
            "train loss:0.021968614184180845\n",
            "train loss:0.02056495187966008\n",
            "train loss:0.017078205782439144\n",
            "train loss:0.02523803318451923\n",
            "train loss:0.014321834161503405\n",
            "train loss:0.005949263262421497\n",
            "train loss:0.009046845220795682\n",
            "train loss:0.044069797269193335\n",
            "train loss:0.008784611846162494\n",
            "train loss:0.008442761412981576\n",
            "train loss:0.011141667908411236\n",
            "train loss:0.01960354014851311\n",
            "train loss:0.01781376621305363\n",
            "train loss:0.018299688417364096\n",
            "train loss:0.08305418726971642\n",
            "train loss:0.010664210829178417\n",
            "train loss:0.005767621441414715\n",
            "train loss:0.03342380985501991\n",
            "train loss:0.020727941692961088\n",
            "train loss:0.030251449439780233\n",
            "train loss:0.021921306301575162\n",
            "train loss:0.018499168103510745\n",
            "train loss:0.020826958405829286\n",
            "train loss:0.020593823344866365\n",
            "train loss:0.012162495738564746\n",
            "train loss:0.03532955903552099\n",
            "train loss:0.021788949104010052\n",
            "train loss:0.025707480969631585\n",
            "train loss:0.016368793616583777\n",
            "train loss:0.02580249227266599\n",
            "train loss:0.009056898363674003\n",
            "train loss:0.036835144041330904\n",
            "train loss:0.01344994294513053\n",
            "=== epoch:12, train acc:0.995, test acc:0.959 ===\n",
            "train loss:0.016053470227004697\n",
            "train loss:0.04036701116622785\n",
            "train loss:0.021674143083540865\n",
            "train loss:0.017025755196772168\n",
            "train loss:0.004523339705477791\n",
            "train loss:0.01401398193628102\n",
            "train loss:0.005623094177274037\n",
            "train loss:0.015162242395330783\n",
            "train loss:0.011240030676476642\n",
            "train loss:0.01095222757447023\n",
            "train loss:0.0124015164033374\n",
            "train loss:0.01832638177678906\n",
            "train loss:0.0881882290120033\n",
            "train loss:0.008739067772037009\n",
            "train loss:0.017866715674901328\n",
            "train loss:0.09544118397323287\n",
            "train loss:0.008887529665735317\n",
            "train loss:0.009289378414402714\n",
            "train loss:0.011883246925026823\n",
            "train loss:0.010923972804047985\n",
            "train loss:0.012389800062679342\n",
            "train loss:0.012829325671743446\n",
            "train loss:0.03348047013029418\n",
            "train loss:0.018006123101918278\n",
            "train loss:0.02872448269369412\n",
            "train loss:0.00804384024532832\n",
            "train loss:0.015225358420289754\n",
            "train loss:0.018532271645701905\n",
            "train loss:0.013145417593719472\n",
            "train loss:0.022945333723964092\n",
            "train loss:0.023855234272142773\n",
            "train loss:0.0057283410770024214\n",
            "train loss:0.038627955751636835\n",
            "train loss:0.011403387237336684\n",
            "train loss:0.013025171193575769\n",
            "train loss:0.013705892957942174\n",
            "train loss:0.04975381603144183\n",
            "train loss:0.024284041555206127\n",
            "train loss:0.02143689574568538\n",
            "train loss:0.012771139241446662\n",
            "train loss:0.013063211567375943\n",
            "train loss:0.018801902771845715\n",
            "train loss:0.01974909750108407\n",
            "train loss:0.019700156833523025\n",
            "train loss:0.027432826612484\n",
            "train loss:0.042715865215222484\n",
            "train loss:0.020937810265968144\n",
            "train loss:0.009741766951081272\n",
            "train loss:0.010780704448304949\n",
            "train loss:0.018273767757714953\n",
            "=== epoch:13, train acc:0.994, test acc:0.961 ===\n",
            "train loss:0.023472914804300738\n",
            "train loss:0.011044864442994377\n",
            "train loss:0.024672971702826434\n",
            "train loss:0.016430460834547654\n",
            "train loss:0.01742263858143194\n",
            "train loss:0.01474589788370944\n",
            "train loss:0.055407425351601106\n",
            "train loss:0.015820159808213538\n",
            "train loss:0.061830648304660035\n",
            "train loss:0.01803815499528038\n",
            "train loss:0.01924021142653496\n",
            "train loss:0.009825962091867342\n",
            "train loss:0.02458665141014796\n",
            "train loss:0.007839475634879467\n",
            "train loss:0.00891401395051152\n",
            "train loss:0.019661023189170224\n",
            "train loss:0.030546522252105638\n",
            "train loss:0.016406250428470055\n",
            "train loss:0.014593380290717406\n",
            "train loss:0.013247266274561666\n",
            "train loss:0.01986612333740019\n",
            "train loss:0.015683373449156525\n",
            "train loss:0.013698322356732457\n",
            "train loss:0.013411828320002308\n",
            "train loss:0.02929956117311182\n",
            "train loss:0.018618357303818323\n",
            "train loss:0.009851408840815245\n",
            "train loss:0.009613188269183894\n",
            "train loss:0.03104314800110784\n",
            "train loss:0.00839347797183771\n",
            "train loss:0.02021149202988753\n",
            "train loss:0.006492186993788875\n",
            "train loss:0.007949511713584333\n",
            "train loss:0.017134968257065307\n",
            "train loss:0.013088211035524634\n",
            "train loss:0.010502529583773543\n",
            "train loss:0.0033076765683556293\n",
            "train loss:0.00450133182492771\n",
            "train loss:0.010179933093377952\n",
            "train loss:0.009747052775261078\n",
            "train loss:0.0028392685982960354\n",
            "train loss:0.008675245553955662\n",
            "train loss:0.005786714038151387\n",
            "train loss:0.007815423394949703\n",
            "train loss:0.014604716139132938\n",
            "train loss:0.018571843545913284\n",
            "train loss:0.0036362636648427397\n",
            "train loss:0.005274937845739319\n",
            "train loss:0.006424987705655671\n",
            "train loss:0.005513887384484121\n",
            "=== epoch:14, train acc:0.998, test acc:0.968 ===\n",
            "train loss:0.005916292361709125\n",
            "train loss:0.01006104607005382\n",
            "train loss:0.004686825437003769\n",
            "train loss:0.006843607954594337\n",
            "train loss:0.016175465114680352\n",
            "train loss:0.006704986396358142\n",
            "train loss:0.016136303271235452\n",
            "train loss:0.006438021862697803\n",
            "train loss:0.011215913616409052\n",
            "train loss:0.016553577352886494\n",
            "train loss:0.010388476013306908\n",
            "train loss:0.007766380666474323\n",
            "train loss:0.008103562447069343\n",
            "train loss:0.007512655520607234\n",
            "train loss:0.0202319391642884\n",
            "train loss:0.008290573554492073\n",
            "train loss:0.007393995516659576\n",
            "train loss:0.003827347318626766\n",
            "train loss:0.016340485677386236\n",
            "train loss:0.0029952901731215885\n",
            "train loss:0.006480571388632392\n",
            "train loss:0.0032166550143221057\n",
            "train loss:0.016715357338701786\n",
            "train loss:0.00809815178804595\n",
            "train loss:0.008049143354761755\n",
            "train loss:0.005145169807694667\n",
            "train loss:0.00864258787459747\n",
            "train loss:0.015394444101659974\n",
            "train loss:0.0044688801624126525\n",
            "train loss:0.004977616060874095\n",
            "train loss:0.015120939112648122\n",
            "train loss:0.007366068136626103\n",
            "train loss:0.003315480676312764\n",
            "train loss:0.004611218363185862\n",
            "train loss:0.007670682442297579\n",
            "train loss:0.007295930054170647\n",
            "train loss:0.006839737944453438\n",
            "train loss:0.006779767083003506\n",
            "train loss:0.013593970979320157\n",
            "train loss:0.0053306634366073035\n",
            "train loss:0.0025870513477119442\n",
            "train loss:0.005787206446855908\n",
            "train loss:0.00572010414131114\n",
            "train loss:0.004126317868102036\n",
            "train loss:0.012013168177749462\n",
            "train loss:0.008705892179843737\n",
            "train loss:0.01198901586877069\n",
            "train loss:0.006725970123306078\n",
            "train loss:0.005338210174952724\n",
            "train loss:0.012367680432282589\n",
            "=== epoch:15, train acc:0.998, test acc:0.967 ===\n",
            "train loss:0.002951074668110192\n",
            "train loss:0.003727948802797852\n",
            "train loss:0.006725602693615804\n",
            "train loss:0.004136165984174641\n",
            "train loss:0.01417890403746897\n",
            "train loss:0.00866453331340977\n",
            "train loss:0.005561909650227031\n",
            "train loss:0.004722864092646929\n",
            "train loss:0.00566370981121845\n",
            "train loss:0.0036831678084900286\n",
            "train loss:0.004660414221680097\n",
            "train loss:0.018421305084678697\n",
            "train loss:0.005350204562548449\n",
            "train loss:0.008137109664332108\n",
            "train loss:0.017159540778950967\n",
            "train loss:0.013190757997565215\n",
            "train loss:0.002504284243052076\n",
            "train loss:0.00446787095020103\n",
            "train loss:0.0029356590449985632\n",
            "train loss:0.0037799744489842653\n",
            "train loss:0.010772651505018766\n",
            "train loss:0.014411164667264473\n",
            "train loss:0.004155982902341834\n",
            "train loss:0.01448487282692192\n",
            "train loss:0.001560010343274678\n",
            "train loss:0.009326066886005804\n",
            "train loss:0.005154226020062459\n",
            "train loss:0.02991673575656406\n",
            "train loss:0.0060189370189935336\n",
            "train loss:0.020456825728523042\n",
            "train loss:0.0121315694699816\n",
            "train loss:0.0064428404387975695\n",
            "train loss:0.007811803523750657\n",
            "train loss:0.007880008383332561\n",
            "train loss:0.008376377237983033\n",
            "train loss:0.006303524277281263\n",
            "train loss:0.011007959535034988\n",
            "train loss:0.005502334173573781\n",
            "train loss:0.004296276774460473\n",
            "train loss:0.01115100676899253\n",
            "train loss:0.012103547012487536\n",
            "train loss:0.004376362629370414\n",
            "train loss:0.004538545833767605\n",
            "train loss:0.005644043176383633\n",
            "train loss:0.0014577569767845594\n",
            "train loss:0.016368192072899284\n",
            "train loss:0.011460348747741085\n",
            "train loss:0.0035867860567422973\n",
            "train loss:0.00696459800465825\n",
            "train loss:0.0024900305860219335\n",
            "=== epoch:16, train acc:0.999, test acc:0.971 ===\n",
            "train loss:0.0037486494409176836\n",
            "train loss:0.004236957472731277\n",
            "train loss:0.004725641076152452\n",
            "train loss:0.004608186820156786\n",
            "train loss:0.005170214026017053\n",
            "train loss:0.007330555664771622\n",
            "train loss:0.00833889490512846\n",
            "train loss:0.0031639916353347164\n",
            "train loss:0.0074316545619518585\n",
            "train loss:0.015451634559412333\n",
            "train loss:0.002113617091899417\n",
            "train loss:0.0050491515309565495\n",
            "train loss:0.005487923126699167\n",
            "train loss:0.005111671997155014\n",
            "train loss:0.008397429687449848\n",
            "train loss:0.0035293206698268857\n",
            "train loss:0.003242844205204312\n",
            "train loss:0.0033083866260595605\n",
            "train loss:0.008913744242075002\n",
            "train loss:0.006376107816083789\n",
            "train loss:0.0026200984782797213\n",
            "train loss:0.0038453110091117098\n",
            "train loss:0.01756960683470509\n",
            "train loss:0.0050040536972295645\n",
            "train loss:0.005994165170515511\n",
            "train loss:0.0016652118287315445\n",
            "train loss:0.001217055987156723\n",
            "train loss:0.0032987858663043753\n",
            "train loss:0.008922021050382171\n",
            "train loss:0.018329582990954794\n",
            "train loss:0.0054136168981118804\n",
            "train loss:0.001917530518490661\n",
            "train loss:0.0015972127409541014\n",
            "train loss:0.0027056035135698954\n",
            "train loss:0.0039554344013297184\n",
            "train loss:0.0028921693600156406\n",
            "train loss:0.0020020807035251993\n",
            "train loss:0.001804803560211048\n",
            "train loss:0.019502352595494193\n",
            "train loss:0.008594421926930525\n",
            "train loss:0.012012458487895896\n",
            "train loss:0.0033346176667733473\n",
            "train loss:0.004070178127651585\n",
            "train loss:0.00538843992866832\n",
            "train loss:0.009371635244988314\n",
            "train loss:0.015467482835551955\n",
            "train loss:0.0032326181245307182\n",
            "train loss:0.008708074619162122\n",
            "train loss:0.0018619646049260203\n",
            "train loss:0.003707076478214293\n",
            "=== epoch:17, train acc:0.998, test acc:0.968 ===\n",
            "train loss:0.004466691656588141\n",
            "train loss:0.007248065894728173\n",
            "train loss:0.002258331789113314\n",
            "train loss:0.01169057329586679\n",
            "train loss:0.007258945677145871\n",
            "train loss:0.004693593636402352\n",
            "train loss:0.011324439594980494\n",
            "train loss:0.003608236666094585\n",
            "train loss:0.011396755186705153\n",
            "train loss:0.0070721449940919605\n",
            "train loss:0.0021569541448020545\n",
            "train loss:0.003504443627221785\n",
            "train loss:0.0010495571004065722\n",
            "train loss:0.005695478608923968\n",
            "train loss:0.009399628768851162\n",
            "train loss:0.002590235043888207\n",
            "train loss:0.00303942947891199\n",
            "train loss:0.005596020216939124\n",
            "train loss:0.0021213503489668057\n",
            "train loss:0.002087680949779277\n",
            "train loss:0.002508058409352885\n",
            "train loss:0.007645726824414937\n",
            "train loss:0.0021713898681615267\n",
            "train loss:0.006534470995995641\n",
            "train loss:0.004043104436199616\n",
            "train loss:0.0036602086198194664\n",
            "train loss:0.003159459655547328\n",
            "train loss:0.0024622740473536045\n",
            "train loss:0.002724599422431376\n",
            "train loss:0.0021163198079720115\n",
            "train loss:0.0037688209109908092\n",
            "train loss:0.005280600888203207\n",
            "train loss:0.0010477994180051724\n",
            "train loss:0.006917713113091795\n",
            "train loss:0.002823712868220254\n",
            "train loss:0.003678130010917048\n",
            "train loss:0.024897414943553672\n",
            "train loss:0.0032861620460223565\n",
            "train loss:0.0029293693911109043\n",
            "train loss:0.0027365785453402507\n",
            "train loss:0.0023092133030495343\n",
            "train loss:0.003809399724772108\n",
            "train loss:0.0020408058364331915\n",
            "train loss:0.0021665300039032114\n",
            "train loss:0.0033161025349467744\n",
            "train loss:0.002297817640131915\n",
            "train loss:0.002399169216003061\n",
            "train loss:0.003620598289232525\n",
            "train loss:0.003016096889093943\n",
            "train loss:0.002501018336927989\n",
            "=== epoch:18, train acc:1.0, test acc:0.969 ===\n",
            "train loss:0.002464914025864551\n",
            "train loss:0.0008070667169953253\n",
            "train loss:0.003963949403633124\n",
            "train loss:0.0014696158533369892\n",
            "train loss:0.0012057385782487818\n",
            "train loss:0.005428384570337608\n",
            "train loss:0.0022501423175093262\n",
            "train loss:0.0005495960021576483\n",
            "train loss:0.0014920256609020084\n",
            "train loss:0.0011674123075405254\n",
            "train loss:0.005890849968184758\n",
            "train loss:0.002037312495891143\n",
            "train loss:0.0025145160495973636\n",
            "train loss:0.0035292477111974713\n",
            "train loss:0.0033530456726167312\n",
            "train loss:0.004823017345515555\n",
            "train loss:0.0031140830269625348\n",
            "train loss:0.007270303180579407\n",
            "train loss:0.002814099578641485\n",
            "train loss:0.0025206989247759503\n",
            "train loss:0.0013420545606402515\n",
            "train loss:0.00455613537741755\n",
            "train loss:0.0010635955221428774\n",
            "train loss:0.002122201167970958\n",
            "train loss:0.008051371746124381\n",
            "train loss:0.0015194331775411748\n",
            "train loss:0.006217213830293679\n",
            "train loss:0.002243065372313315\n",
            "train loss:0.0008027342840572191\n",
            "train loss:0.01113274970154471\n",
            "train loss:0.007239013251166518\n",
            "train loss:0.0061195186893919585\n",
            "train loss:0.010593234219976775\n",
            "train loss:0.003850659651906985\n",
            "train loss:0.005427393763924415\n",
            "train loss:0.002158358588529403\n",
            "train loss:0.003643784272245688\n",
            "train loss:0.001749426784841109\n",
            "train loss:0.004399962990438692\n",
            "train loss:0.0034503139725818306\n",
            "train loss:0.004072975071192002\n",
            "train loss:0.003032929092101071\n",
            "train loss:0.0013832163931838746\n",
            "train loss:0.0030844864530551546\n",
            "train loss:0.0025127860680653515\n",
            "train loss:0.006101371978832798\n",
            "train loss:0.0020632351052476917\n",
            "train loss:0.0033734271813487793\n",
            "train loss:0.0021043361122913906\n",
            "train loss:0.0012775346523822989\n",
            "=== epoch:19, train acc:1.0, test acc:0.972 ===\n",
            "train loss:0.0026691712284600717\n",
            "train loss:0.0023508241665309544\n",
            "train loss:0.0015606071869731914\n",
            "train loss:0.0014930249380660193\n",
            "train loss:0.0009405984733769019\n",
            "train loss:0.005408843937835103\n",
            "train loss:0.0034425746338130926\n",
            "train loss:0.00434108598460525\n",
            "train loss:0.0014002567729701747\n",
            "train loss:0.0023009271001047733\n",
            "train loss:0.00179049497775947\n",
            "train loss:0.002271178316216472\n",
            "train loss:0.0076395137287226566\n",
            "train loss:0.001315303228997779\n",
            "train loss:0.0035005688952594745\n",
            "train loss:0.00650127361050359\n",
            "train loss:0.0034453513541615704\n",
            "train loss:0.0012720570657807774\n",
            "train loss:0.0020178655570677606\n",
            "train loss:0.004471041953466292\n",
            "train loss:0.005046001585127112\n",
            "train loss:0.0032665520753713457\n",
            "train loss:0.0034659038619567154\n",
            "train loss:0.002929220684693254\n",
            "train loss:0.0029415980016287728\n",
            "train loss:0.005264107173429611\n",
            "train loss:0.004134885151781385\n",
            "train loss:0.0017992064253825445\n",
            "train loss:0.005205954315993846\n",
            "train loss:0.00175714787388232\n",
            "train loss:0.002927696058461518\n",
            "train loss:0.0013071250011986143\n",
            "train loss:0.0028097687496257094\n",
            "train loss:0.0024470949869714056\n",
            "train loss:0.00355520119607267\n",
            "train loss:0.003265607964176857\n",
            "train loss:0.006254950065552348\n",
            "train loss:0.003231404903223223\n",
            "train loss:0.007029166476454288\n",
            "train loss:0.0006093351464535946\n",
            "train loss:0.0038252736224781846\n",
            "train loss:0.0027159284741279537\n",
            "train loss:0.0005290530494077539\n",
            "train loss:0.000919187890093347\n",
            "train loss:0.0039383329434094\n",
            "train loss:0.0009030724805243729\n",
            "train loss:0.004235658535427686\n",
            "train loss:0.0018694970813000005\n",
            "train loss:0.0004199273677019595\n",
            "train loss:0.0034857762229047917\n",
            "=== epoch:20, train acc:1.0, test acc:0.968 ===\n",
            "train loss:0.0018065713638832416\n",
            "train loss:0.004912497084550858\n",
            "train loss:0.005208062492438129\n",
            "train loss:0.0013427837677199015\n",
            "train loss:0.001531629891256564\n",
            "train loss:0.001514118154293075\n",
            "train loss:0.007527242883132046\n",
            "train loss:0.0016245128848678045\n",
            "train loss:0.0012231888723100107\n",
            "train loss:0.0016153659217532992\n",
            "train loss:0.006460826165226824\n",
            "train loss:0.0016261436962551837\n",
            "train loss:0.004884318218504813\n",
            "train loss:0.0016696945245968343\n",
            "train loss:0.0029041379732739213\n",
            "train loss:0.00303989985836079\n",
            "train loss:0.0018472264095773078\n",
            "train loss:0.003228650441866781\n",
            "train loss:0.002290171871457985\n",
            "train loss:0.0010229354679986793\n",
            "train loss:0.0008965745439398727\n",
            "train loss:0.0011014298707703015\n",
            "train loss:0.0041062059150773645\n",
            "train loss:0.00226997430876254\n",
            "train loss:0.000384626352145051\n",
            "train loss:0.001392194643449783\n",
            "train loss:0.0013510816856045671\n",
            "train loss:0.00166188730496139\n",
            "train loss:0.007467341565367233\n",
            "train loss:0.001027650534750597\n",
            "train loss:0.0025724115627384736\n",
            "train loss:0.0012354837813330604\n",
            "train loss:0.0014504107550880085\n",
            "train loss:0.001562062346568458\n",
            "train loss:0.0028805114025285145\n",
            "train loss:0.002388195216983569\n",
            "train loss:0.003521099016349422\n",
            "train loss:0.0019076114515655814\n",
            "train loss:0.0015018452527717464\n",
            "train loss:0.0019458000766955294\n",
            "train loss:0.0026643726366087977\n",
            "train loss:0.004341894701596041\n",
            "train loss:0.0037372089436886733\n",
            "train loss:0.002377772941190327\n",
            "train loss:0.0016121315568047958\n",
            "train loss:0.0010506654144525368\n",
            "train loss:0.0016364841749587824\n",
            "train loss:0.0008736952778600937\n",
            "train loss:0.002791739865257892\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.971\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch06.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# \n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = False)\n",
        "\n",
        "#      .\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim =(1, 28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=500, output_size=10, weight_init_std = 0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                    epochs = max_epochs, mini_batch_size = 100,\n",
        "                    optimizer = 'Adam', optimizer_param={'lr': 0.001},\n",
        "                    evaluate_sample_num_per_epoch=1000)\n",
        "                    \n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# \n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# \n",
        "markers = {'train' : 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label ='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
